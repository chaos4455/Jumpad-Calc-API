# -*- coding: utf-8 -*-
"""
Script de Testes UnitÃ¡rios para a Biblioteca calc_numbers (Classe Numbers)

Este script Python implementa testes unitÃ¡rios abrangentes para a biblioteca
`calc_numbers`, especificamente para a classe `Numbers`. Utiliza o framework `pytest`
para definir e executar os testes, e registra os resultados detalhados em arquivos
de log JSON para anÃ¡lise e rastreamento.

Os testes unitÃ¡rios cobrem as seguintes funcionalidades da classe `Numbers`:

    - MÃ©todo `sum_numbers`: Testes para diferentes cenÃ¡rios de entrada, incluindo listas
      de inteiros vÃ¡lidos, strings numÃ©ricas, listas vazias, tipos invÃ¡lidos, listas com
      strings nÃ£o numÃ©ricas, listas com floats (com e sem perda de precisÃ£o), e listas com None.

    - MÃ©todo `calculate_average`: Testes similares ao `sum_numbers`, mas focados no cÃ¡lculo
      da mÃ©dia, incluindo casos de lista vazia (que deve retornar None), divisÃ£o por zero
      (comportamento esperado para listas de zeros), e os mesmos cenÃ¡rios de validaÃ§Ã£o de entrada
      aplicados ao `sum_numbers`.

O script inclui funcionalidades para:

    - GeraÃ§Ã£o de logs de teste em formato JSON, com timestamps e detalhes de cada caso de teste.
    - OrganizaÃ§Ã£o dos logs em diretÃ³rios, com nomes de arquivos sequenciais para evitar sobrescrita.
    - Uso de `pytest.raises` para testar o lanÃ§amento correto de exceÃ§Ãµes esperadas.
    - Uso de `pytest.approx` para comparaÃ§Ã£o aproximada de nÃºmeros de ponto flutuante em testes de mÃ©dia.
    - Mensagens informativas no console sobre o inÃ­cio e a conclusÃ£o dos testes, e o local dos logs.

Para executar os testes, basta rodar este script Python diretamente. Os resultados
detalhados serÃ£o salvos em arquivos JSON no diretÃ³rio `test_logs/`.

Bibliotecas Utilizadas:
    - pytest: Framework de testes Python para executar e organizar os testes unitÃ¡rios.
    - os: Para manipulaÃ§Ã£o de arquivos e diretÃ³rios (e.g., criaÃ§Ã£o de diretÃ³rio de logs).
    - json: Para serializaÃ§Ã£o e escrita dos resultados dos testes em formato JSON.
    - datetime: Para geraÃ§Ã£o de timestamps para os logs de teste.
    - bibliotecas.calc_numbers.Numbers: A biblioteca e classe `Numbers` sendo testadas.

DiretÃ³rios e Arquivos Gerados:
    - test_logs/: DiretÃ³rio onde os arquivos de log JSON sÃ£o salvos.
        - log_teste_*.json: Arquivos de log individuais para cada execuÃ§Ã£o de teste, contendo
          os resultados detalhados de cada caso de teste, status (PASSOU, FALHOU, ERRO),
          entradas, saÃ­das esperadas e reais, e informaÃ§Ãµes de exceÃ§Ãµes (se aplicÃ¡vel).

ExecuÃ§Ã£o:
    Execute este script diretamente para iniciar os testes unitÃ¡rios da biblioteca `calc_numbers`.
    Os logs serÃ£o gerados no diretÃ³rio `test_logs/`.

Criado por: Elias Andrade
Data de CriaÃ§Ã£o: 10 de MarÃ§o de 2025
"""
import sys
import os
import json
import datetime
import pytest # ğŸ§ª Framework de testes pytest
from bibliotecas.calc_numbers import Numbers # ğŸ“š Importa a classe Numbers a ser testada

DIRETORIO_LOGS_TESTE = "test_logs" # ğŸ—‚ï¸ DiretÃ³rio para salvar os logs de teste
if not os.path.exists(DIRETORIO_LOGS_TESTE): # ğŸ“ Cria o diretÃ³rio de logs se nÃ£o existir
    os.makedirs(DIRETORIO_LOGS_TESTE)

def obter_proximo_nome_arquivo_log():
    """
    Gera um nome de arquivo de log Ãºnico e sequencial no diretÃ³rio de logs.

    Verifica sequencialmente a existÃªncia de arquivos de log com nomes como 'log_teste_1.json',
    'log_teste_2.json', etc., e retorna o primeiro nome de arquivo que nÃ£o existir, garantindo
    que cada execuÃ§Ã£o de teste tenha seu prÃ³prio arquivo de log, sem sobrescrever logs anteriores.

    Returns:
        str: O caminho completo para o prÃ³ximo arquivo de log disponÃ­vel.
    """
    contador_logs = 1 # ğŸ”¢ Inicia o contador para nomes de arquivos de log
    while True: # ğŸ”„ Loop infinito atÃ© encontrar um nome de arquivo disponÃ­vel
        nome_arquivo_log = os.path.join(DIRETORIO_LOGS_TESTE, f"log_teste_{contador_logs}.json") # ğŸ“ Monta o nome do arquivo de log com contador sequencial
        if not os.path.exists(nome_arquivo_log): # âœ… Verifica se o arquivo de log jÃ¡ existe
            return nome_arquivo_log # ğŸš€ Retorna o nome de arquivo disponÃ­vel encontrado
        contador_logs += 1 # â• Incrementa o contador para tentar o prÃ³ximo nome de arquivo

def salvar_log_teste(dados_log):
    """
    Salva os dados de log de teste em um arquivo JSON.

    Recebe um dicionÃ¡rio contendo os dados de log e escreve-os em um arquivo JSON formatado.
    O nome do arquivo Ã© gerado automaticamente por `obter_proximo_nome_arquivo_log()` para
    garantir que seja Ãºnico e sequencial. Imprime uma mensagem no console informando o local
    onde os resultados dos testes foram salvos.

    Args:
        dados_log (dict): Um dicionÃ¡rio contendo os resultados e detalhes dos testes a serem salvos.
    """
    nome_arquivo_log = obter_proximo_nome_arquivo_log() # ğŸ“ ObtÃ©m um nome de arquivo de log Ãºnico
    with open(nome_arquivo_log, 'w', encoding='utf-8') as arquivo: # ğŸ“ Abre o arquivo de log em modo de escrita ('w') com encoding UTF-8
        json.dump(dados_log, arquivo, indent=4, ensure_ascii=False) # ğŸ’¾ Escreve os dados de log em formato JSON formatado no arquivo
    print(f"Resultados dos testes salvos em: {nome_arquivo_log}") # ğŸ’¬ Imprime mensagem informando onde os logs foram salvos

def executar_testes():
    """
    Executa a suÃ­te de testes para a biblioteca calc_numbers, abrangendo soma e mÃ©dia.

    Inicializa a classe `Numbers`, define casos de teste para os mÃ©todos `sum_numbers` e
    `calculate_average`, executa cada caso de teste chamando `executar_caso_teste`, e
    agrega os resultados. Ao final, prepara os dados de log, incluindo um timestamp de
    execuÃ§Ã£o, e chama `salvar_log_teste` para persistir os resultados em um arquivo JSON.
    """
    calculadora = Numbers() # â• Instancia a classe Numbers para executar os testes
    resultados_teste = [] # ğŸ“ Lista para armazenar os resultados de cada caso de teste

    casos_teste_soma = [ # ğŸ§ª Casos de teste para o mÃ©todo sum_numbers
        {"nome": "soma_inteiros_validos", "entrada": [1, 2, 3, 4], "saida_esperada": 10, "espera_excecao": None}, # âœ… Teste de soma com inteiros vÃ¡lidos
        {"nome": "soma_strings_numericas_validas", "entrada": ["5", "10", "15"], "saida_esperada": 30, "espera_excecao": None}, # âœ… Teste de soma com strings numÃ©ricas vÃ¡lidas
        {"nome": "soma_lista_vazia", "entrada": [], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de soma com lista vazia (espera ValueError)
        {"nome": "soma_tipo_invalido_string", "entrada": "nao_eh_lista", "saida_esperada": None, "espera_excecao": TypeError}, # âŒ Teste de soma com tipo de entrada invÃ¡lido (string, espera TypeError)
        {"nome": "soma_lista_com_string", "entrada": [1, 2, "a", 4], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de soma com lista contendo string nÃ£o numÃ©rica (espera ValueError)
        {"nome": "soma_lista_com_float_sem_perda", "entrada": [1, 2, 3.0, 4], "saida_esperada": 10, "espera_excecao": None}, # âœ… Teste de soma com lista contendo float sem perda de precisÃ£o
        {"nome": "soma_lista_com_float_com_perda", "entrada": [1, 2, 3.5, 4], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de soma com lista contendo float com perda de precisÃ£o (espera ValueError)
        {"nome": "soma_lista_com_none", "entrada": [1, 2, None, 4], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de soma com lista contendo None (espera ValueError)
    ]

    for caso in casos_teste_soma: # ğŸ”„ Executa cada caso de teste para a funÃ§Ã£o sum_numbers
        resultado = executar_caso_teste(calculadora.sum_numbers, caso) # ğŸ§ª Executa o caso de teste e obtÃ©m o resultado
        resultados_teste.append(resultado) # ğŸ“ Adiciona o resultado do teste Ã  lista de resultados

    casos_teste_media = [ # ğŸ§ª Casos de teste para o mÃ©todo calculate_average (similar aos de soma, mas para mÃ©dia)
        {"nome": "media_inteiros_validos", "entrada": [1, 2, 3, 4], "saida_esperada": 2.5, "espera_excecao": None}, # âœ… Teste de mÃ©dia com inteiros vÃ¡lidos
        {"nome": "media_strings_numericas_validas", "entrada": ["5", "10", "15"], "saida_esperada": 10.0, "espera_excecao": None}, # âœ… Teste de mÃ©dia com strings numÃ©ricas vÃ¡lidas
        {"nome": "media_lista_vazia", "entrada": [], "saida_esperada": None, "espera_excecao": None}, # âœ… Teste de mÃ©dia com lista vazia (espera retorno None)
        {"nome": "media_tipo_invalido_string", "entrada": "nao_eh_lista", "saida_esperada": None, "espera_excecao": TypeError}, # âŒ Teste de mÃ©dia com tipo de entrada invÃ¡lido (string, espera TypeError)
        {"nome": "media_lista_com_string", "entrada": [1, 2, "a", 4], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de mÃ©dia com lista contendo string nÃ£o numÃ©rica (espera ValueError)
        {"nome": "media_lista_com_float_sem_perda", "entrada": [1, 2, 3.0, 4], "saida_esperada": 2.5, "espera_excecao": None}, # âœ… Teste de mÃ©dia com lista contendo float sem perda de precisÃ£o
        {"nome": "media_lista_com_float_com_perda", "entrada": [1, 2, 3.5, 4], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de mÃ©dia com lista contendo float com perda de precisÃ£o (espera ValueError)
        {"nome": "media_lista_com_none", "entrada": [1, 2, None, 4], "saida_esperada": None, "espera_excecao": ValueError}, # âŒ Teste de mÃ©dia com lista contendo None (espera ValueError)
        {"nome": "media_divisao_por_zero", "entrada": [0, 0, 0], "saida_esperada": 0.0, "espera_excecao": None}, # âœ… Teste de mÃ©dia com lista de zeros (espera mÃ©dia 0.0)
    ]

    for caso in casos_teste_media: # ğŸ”„ Executa cada caso de teste para a funÃ§Ã£o calculate_average
        resultado = executar_caso_teste(calculadora.calculate_average, caso) # ğŸ§ª Executa o caso de teste e obtÃ©m o resultado
        resultados_teste.append(resultado) # ğŸ“ Adiciona o resultado do teste Ã  lista de resultados

    dados_log = {"timestamp_execucao_teste": datetime.datetime.now().isoformat(), "resultados_teste": resultados_teste} # ğŸ“ Prepara os dados de log para serem salvos em arquivo JSON
    salvar_log_teste(dados_log) # ğŸ’¾ Salva os dados de log em arquivo JSON

def executar_caso_teste(funcao_testar, caso_teste):
    """
    Executa um Ãºnico caso de teste para uma funÃ§Ã£o especÃ­fica (sum_numbers ou calculate_average).

    Recebe a funÃ§Ã£o a ser testada e um dicionÃ¡rio contendo os detalhes do caso de teste
    (nome, entrada, saÃ­da esperada, exceÃ§Ã£o esperada). Executa a funÃ§Ã£o com a entrada fornecida
    e compara o resultado com a saÃ­da esperada, ou verifica se a exceÃ§Ã£o esperada Ã© lanÃ§ada.
    Registra o status do teste (PASSOU, FALHOU, ERRO) e detalhes adicionais em um dicionÃ¡rio
    de log para posterior anÃ¡lise.

    Args:
        funcao_testar (callable): A funÃ§Ã£o da classe Numbers a ser testada (sum_numbers ou calculate_average).
        caso_teste (dict): Um dicionÃ¡rio contendo os parÃ¢metros do caso de teste, incluindo:
            - "nome" (str): Nome descritivo do teste.
            - "entrada" (any): Dados de entrada para a funÃ§Ã£o de teste.
            - "saida_esperada" (any): Valor esperado de retorno da funÃ§Ã£o em caso de sucesso.
            - "espera_excecao" (Exception): Tipo de exceÃ§Ã£o esperada, se o teste esperar que uma exceÃ§Ã£o seja lanÃ§ada.

    Returns:
        dict: Um dicionÃ¡rio contendo os resultados detalhados do caso de teste, incluindo:
            - "nome_teste" (str): Nome do teste.
            - "entrada" (any): Dados de entrada do teste.
            - "saida_esperada" (any): SaÃ­da esperada.
            - "excecao_esperada" (str): Nome da exceÃ§Ã£o esperada (se houver).
            - "saida_real" (any): SaÃ­da real obtida da funÃ§Ã£o testada.
            - "status_teste" (str): Status do teste ("PASSOU", "FALHOU", "ERRO").
            - "excecao_real" (str, opcional): Nome da exceÃ§Ã£o real lanÃ§ada (se aplicÃ¡vel).
            - "mensagem_excecao" (str, opcional): Mensagem da exceÃ§Ã£o lanÃ§ada (se aplicÃ¡vel).
            - "mensagem_erro" (str, opcional): Mensagem de erro genÃ©rico (se ocorrer um erro inesperado).
    """
    nome_teste = caso_teste["nome"] # ğŸ·ï¸ Nome do caso de teste
    dados_entrada = caso_teste["entrada"] # ğŸ“¦ Dados de entrada para o teste
    saida_esperada = caso_teste["saida_esperada"] # ğŸ¯ SaÃ­da esperada do teste
    excecao_esperada = caso_teste["espera_excecao"] # â³ ExceÃ§Ã£o esperada (se houver)

    entrada_log = {"nome_teste": nome_teste, "entrada": dados_entrada, "saida_esperada": saida_esperada, "excecao_esperada": excecao_esperada.__name__ if excecao_esperada else None} # ğŸ“ Inicializa o dicionÃ¡rio de log para este caso de teste

    try: # ğŸ”„ Bloco try-except para capturar exceÃ§Ãµes durante a execuÃ§Ã£o do teste
        if excecao_esperada: # â³ Se o teste espera uma exceÃ§Ã£o
            with pytest.raises(excecao_esperada) as info_excecao: # ğŸ›¡ï¸ Usa pytest.raises para verificar se a exceÃ§Ã£o esperada Ã© lanÃ§ada
                funcao_testar(dados_entrada) # ğŸ§ª Executa a funÃ§Ã£o de teste com os dados de entrada
            saida_real = None # ğŸ”‡ Define saida_real como None, pois espera-se uma exceÃ§Ã£o
            status_teste = "PASSOU" # âœ… Se a exceÃ§Ã£o esperada for lanÃ§ada, o teste passa
            entrada_log["excecao_real"] = info_excecao.type.__name__ # ğŸ“ Registra o nome da exceÃ§Ã£o real lanÃ§ada
            entrada_log["mensagem_excecao"] = str(info_excecao.value) # ğŸ“ Registra a mensagem da exceÃ§Ã£o lanÃ§ada

        else: # âœ… Se o teste nÃ£o espera uma exceÃ§Ã£o (espera um retorno bem-sucedido)
            saida_real = funcao_testar(dados_entrada) # ğŸ§ª Executa a funÃ§Ã£o de teste e obtÃ©m a saÃ­da real
            if saida_esperada is None: # ğŸ¯ Caso esperado seja None (verifica se a saÃ­da real tambÃ©m Ã© None)
                if saida_real is None: # âœ… Se a saÃ­da real for None, o teste passa
                    status_teste = "PASSOU" # âœ… Status do teste: PASSOU
                else: # âŒ Se a saÃ­da real nÃ£o for None, mas esperava-se None, o teste falha
                    status_teste = "FALHOU" # âŒ Status do teste: FALHOU
                    entrada_log["saida_real"] = saida_real # ğŸ“ Registra a saÃ­da real (incorreta)
            elif isinstance(saida_esperada, float): # ğŸ”¢ Caso esperado seja float (usa pytest.approx para comparaÃ§Ã£o aproximada)
                if pytest.approx(saida_real) == saida_esperada: # âœ… Compara a saÃ­da real com a esperada usando pytest.approx para floats
                    status_teste = "PASSOU" # âœ… Status do teste: PASSOU
                else: # âŒ Se a saÃ­da real nÃ£o for aproximadamente igual Ã  esperada, o teste falha
                    status_teste = "FALHOU" # âŒ Status do teste: FALHOU
                    entrada_log["saida_real"] = saida_real # ğŸ“ Registra a saÃ­da real (incorreta)
            else: # ğŸ¯ Caso esperado seja de outro tipo (comparaÃ§Ã£o direta)
                if saida_real == saida_esperada: # âœ… Compara a saÃ­da real com a esperada diretamente
                    status_teste = "PASSOU" # âœ… Status do teste: PASSOU
                else: # âŒ Se a saÃ­da real nÃ£o for igual Ã  esperada, o teste falha
                    status_teste = "FALHOU" # âŒ Status do teste: FALHOU
                    entrada_log["saida_real"] = saida_real # ğŸ“ Registra a saÃ­da real (incorreta)

    except Exception as erro: # â— Captura qualquer exceÃ§Ã£o inesperada durante o teste (fora das exceÃ§Ãµes esperadas)
        status_teste = "ERRO" # â— Status do teste: ERRO (exceÃ§Ã£o inesperada)
        entrada_log["status_teste"] = status_teste # ğŸ“ Registra o status do teste como ERRO
        entrada_log["mensagem_erro"] = str(erro) # ğŸ“ Registra a mensagem do erro inesperado
        saida_real = None # ğŸ”‡ Define saida_real como None em caso de erro

    entrada_log["status_teste"] = status_teste # ğŸ“ Garante que o status do teste seja sempre registrado no log
    return entrada_log # ğŸ“ Retorna o dicionÃ¡rio de log completo para este caso de teste

if __name__ == "__main__":
    print("Iniciando testes da biblioteca calc_numbers...") # ğŸ’¬ Mensagem de inÃ­cio dos testes no console
    executar_testes() # ğŸš€ Executa a funÃ§Ã£o principal de testes
    print("Testes da biblioteca calc_numbers concluÃ­dos e resultados salvos em test_logs/") # ğŸ’¬ Mensagem de conclusÃ£o e local dos logs no console